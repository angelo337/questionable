{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering with PyTorch Transformers: Part 3\n",
    "\n",
    "Read the full article: https://medium.com/@patonw/question-answering-with-pytorch-transformers-part-3-d67ac06a23b7\n",
    "\n",
    "> Welcome back! This is the third part of an on-going series about building a question answering service using the Transformers library. The prior article looked at using scikit-learn to build an indexing service for fetching relevant articles to feed into Transformers.\n",
    ">\n",
    "> This time we’ll really start working with the library in more depth. In this article we’re going to peel back a layer to examine the inner workings of the Transformers question answering pipeline. Then we’ll use the model API to build our own pipeline. Finally we’ll wrap it all up in a simple Flask service that can be accessed over a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for Paperspace. Manage these via conda or pipenv on your own machine\n",
    "!pip --quiet install flask torch transformers sklearn pyarrow seaborn spacy[cuda92]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "import sklearn\n",
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from itertools import islice\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this if you've already run Part 2\n",
    "%run \"Questionable - Part 2.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes a while the first time, since from_pretrained() downloads and caches the model weights\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForQuestionAnswering \\\n",
    "    .from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad') \\\n",
    "    .to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_df = pd.read_feather(\"cache/question_context.feather\")\n",
    "question_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question, context = question_df[[\"question\", \"context\"]].iloc[1]\n",
    "question, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from https://huggingface.co/transformers/model_doc/bert.html#bertforquestionanswering\n",
    "input_text = \"[CLS] \" + question + \" [SEP] \" + context + \" [SEP]\"\n",
    "input_ids = tokenizer.encode(input_text, add_special_tokens=False)\n",
    "token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\n",
    "input_ids[:10], token_type_ids[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    start_scores, end_scores = model(torch.tensor([input_ids], device=device),\n",
    "                                     token_type_ids=torch.tensor([token_type_ids], device=device))\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\n",
    "print(f'score: {torch.max(start_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(start_scores.cpu(), kde=False, rug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = tokenizer.decode(input_ids[torch.argmax(start_scores)-8:torch.argmax(start_scores) ])\n",
    "answer = tokenizer.decode(input_ids[torch.argmax(start_scores) : torch.argmax(end_scores)+1])\n",
    "suffix = tokenizer.decode(input_ids[torch.argmax(end_scores)+1:torch.argmax(end_scores)+8 ])\n",
    "\n",
    "\"...\" + prefix + \" >>>\" + answer + \"<<< \" + suffix + \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question, context = question_df[[\"question\", \"context\"]].iloc[0]\n",
    "question, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from https://huggingface.co/transformers/model_doc/bert.html#bertforquestionanswering\n",
    "input_text = \"[CLS] \" + question + \" [SEP] \" + context + \" [SEP]\"\n",
    "input_ids = tokenizer.encode(input_text, add_special_tokens=False)\n",
    "token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    start_scores, end_scores = model(torch.tensor([input_ids], device=device),\n",
    "                                     token_type_ids=torch.tensor([token_type_ids], device=device))\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\n",
    "print(f'score: {torch.max(start_scores)}')\n",
    "sns.distplot(start_scores.cpu(), kde=False, rug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare batch for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_df[\"encoded\"] = question_df.apply(lambda row: tokenizer.encode(\"[CLS] \" + row[\"question\"] + \" [SEP] \" + row[\"context\"] + \" [SEP]\", add_special_tokens=False), axis=1)\n",
    "question_df[\"tok_type\"] = question_df.apply(lambda row: [0 if i <= row[\"encoded\"].index(102) else 1 for i in range(len(row[\"encoded\"]))], axis=1)\n",
    "question_df.iloc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    X = torch.nn.utils.rnn.pad_sequence([torch.tensor(row) for row in question_df[\"encoded\"]], batch_first=True).to(device)\n",
    "    T = torch.nn.utils.rnn.pad_sequence([torch.tensor(row) for row in question_df[\"tok_type\"]], batch_first=True).to(device)\n",
    "    start_scores, end_scores = model(X, token_type_ids=T)\n",
    "    max_score, max_start = torch.max(start_scores, axis=1)\n",
    "    soft_max = F.softmax(max_score, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_df = question_df[[\"context\", \"encoded\"]].copy()\n",
    "answer_df[\"answer_score\"] = max_score.cpu().numpy()\n",
    "answer_df[\"answer_start\"] = max_start.cpu().numpy()\n",
    "answer_df[\"answer_softmax\"] = soft_max.cpu().numpy()\n",
    "answer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = torch.zeros_like(max_start)\n",
    "for i in range(max_start.shape[0]):\n",
    "    max_len[i] = torch.argmax(end_scores[i,max_start[i]:]) + 1\n",
    "    \n",
    "answer_df[\"answer_length\"] = max_len.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_df = answer_df[answer_df.answer_score > 1.0].sort_values(by=\"answer_score\", ascending=False)\n",
    "answer_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_answer(row):\n",
    "    input_ids = row.encoded\n",
    "    offset = row.answer_start\n",
    "    length = np.clip(row.answer_length, 0, 20)\n",
    "    return tokenizer.decode(input_ids[offset:][:length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_df[\"answer\"] = answer_df.apply(decode_answer, axis=1)\n",
    "answer_df[[\"answer_softmax\", \"answer_score\", \"answer\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_df[[\"answer_softmax\", \"answer_score\", \"answer\", \"context\"]].iloc[:3].to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Questionable Web Service\n",
    "\n",
    "Start a terminal from the jupyter file manager and create a new Terminal to run:\n",
    "\n",
    "```\n",
    "python questionable.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "resp = requests.get(\"http://localhost:8765/answer\", params=dict(\n",
    "  q=\"When did the last country to adopt the Gregorian calendar start using it?\"))\n",
    "resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -G http://localhost:8765/answer \\\n",
    "        --data-urlencode \"q=When did the last country to adopt the Gregorian calendar start using it?\" \\\n",
    "        | json_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
