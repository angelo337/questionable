{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering with PyTorch Transformers: Part 2\n",
    "## A simple vector index with scikit-learn\n",
    "\n",
    "Read the full article: https://medium.com/@patonw/question-answering-with-pytorch-transformers-part-2-a31900294673\n",
    "\n",
    "> In Part 1 we briefly examined the problem of question answering in machine learning and how recent breakthroughs have greatly improved the quality of answers produced by computer systems.\n",
    ">\n",
    "> Using the pipeline API Transformers library we were able to run a pre-trained model in a few lines of code. In this article we’ll prototype an information retrieval system around it. In later articles we’ll turn that into web services that can be queried by browsers and mobile apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for Paperspace. Manage these via conda or pipenv on your own machine\n",
    "!pip --quiet install torch transformers sklearn pyarrow seaborn spacy[cuda92]\n",
    "%run init_container.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qa.constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import sklearn\n",
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from itertools import islice\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.prefer_gpu()\n",
    "sp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Questions from SQUAD2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset if not present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SQUAD_TRAIN) as f:\n",
    "    doc = json.load(f)\n",
    "doc.keys(), type(doc[\"data\"]), len(doc[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = []\n",
    "questions = []\n",
    "for topic in doc[\"data\"]:\n",
    "    for pgraph in topic[\"paragraphs\"]:\n",
    "        paragraphs.append(pgraph[\"context\"])\n",
    "        for qa in pgraph[\"qas\"]:\n",
    "            if not qa[\"is_impossible\"]:\n",
    "                questions.append(qa[\"question\"])\n",
    "        \n",
    "len(paragraphs), len(questions), random.sample(paragraphs, 2), random.sample(questions, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map words to lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(phrase):\n",
    "    return \" \".join([word.lemma_ for word in sp(phrase)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.isfile(LEMMA_CACHE):\n",
    "    lemmas = [lemmatize(par) for par in tqdm(paragraphs)]\n",
    "    df = pd.DataFrame(data={'context': paragraphs, 'lemmas': lemmas})\n",
    "    df.to_feather(LEMMA_CACHE)\n",
    "    \n",
    "df = pd.read_feather(LEMMA_CACHE)\n",
    "paragraphs = df.context\n",
    "lemmas = df.lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_idx = [random.randint(0, len(lemmas)-1) for i in range(10)]\n",
    "\n",
    "# TODO display in left/right columns\n",
    "[(paragraphs[i][:80], lemmas[i][:80]) for i in rand_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize corpus by TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_CACHE = 'cache/vectors.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if not os.path.isfile(VECTOR_CACHE):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words='english', min_df=5, max_df=.5, ngram_range=(1,3))\n",
    "    tfidf = vectorizer.fit_transform(lemmas)\n",
    "    with open(VECTOR_CACHE, \"wb\") as f:\n",
    "        pickle.dump(dict(vectorizer=vectorizer, tfidf=tfidf), f)\n",
    "else:\n",
    "    with open(VECTOR_CACHE, \"rb\") as f:\n",
    "        cache = pickle.load(f)\n",
    "        tfidf = cache[\"tfidf\"]\n",
    "        vectorizer = cache[\"vectorizer\"]\n",
    "        \n",
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch contexts related to question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"When did the last country to adopt the Gregorian calendar start using it?\"\n",
    "query = vectorizer.transform([lemmatize(question)])\n",
    "(query > 0).sum(), vectorizer.inverse_transform(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scores = (tfidf * query.T).toarray()\n",
    "results = (np.flip(np.argsort(scores, axis=0)))\n",
    "[paragraphs[i] for i in results[:3, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract answers from contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qapipe = pipeline('question-answering',\n",
    "                  model='distilbert-base-uncased-distilled-squad',\n",
    "                  tokenizer='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "THRESH = 0.01\n",
    "candidate_idxs = [ (i, scores[i]) for i in results[0:10, 0] ]\n",
    "contexts = [ (paragraphs[i],s)\n",
    "    for (i,s) in candidate_idxs if s > THRESH ]\n",
    "\n",
    "question_df = pd.DataFrame.from_records([ {\n",
    "    'question': question,\n",
    "    'context':  ctx\n",
    "} for (ctx,s) in contexts ])\n",
    "\n",
    "question_df.to_feather(\"cache/question_context.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "preds = qapipe(question_df.to_dict(orient=\"records\"))\n",
    "answer_df = pd.DataFrame.from_records(preds)\n",
    "answer_df[\"context\"] = question_df[\"context\"]\n",
    "answer_df = answer_df.sort_values(by=\"score\", ascending=False)\n",
    "answer_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_df.head().to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
